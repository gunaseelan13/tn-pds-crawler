# This workflow runs the TN PDS Crawler on a schedule and stores results in the repository

name: TN PDS Crawler

on:
  # Run the crawler every day at 11:55 PM IST (18:25 UTC)
  schedule:
    - cron: '25 18 * * *'
  # Allow manual triggering
  workflow_dispatch:
  # Keep the test/lint functionality on push/PR
  push:
    branches: [ "master" ]
  pull_request:
    branches: [ "master" ]

permissions:
  contents: write  # Need write permission to commit results

jobs:
  # Test and lint job
  test:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'pull_request'
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pytest
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    - name: Test with pytest (if tests exist)
      run: |
        python -c "import pytest; import os; exit(0 if len(os.listdir('tests')) == 0 else pytest.main(['tests']))" || true
        
  # Crawler job - runs on schedule or manual trigger
  crawler:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.9
      uses: actions/setup-python@v3
      with:
        python-version: "3.9"
    - name: Setup Chrome and ChromeDriver
      uses: browser-actions/setup-chrome@v1
      with:
        chrome-version: stable
    - name: Setup ChromeDriver
      uses: nanasess/setup-chromedriver@v2
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install selenium webdriver-manager
    - name: Create data directory
      run: mkdir -p data
    - name: Run crawler
      run: |
        python crawai_pds_selenium.py --shop-list-json shop_list.json --output-json data/shop_status_results.json
    - name: Commit and push results
      run: |
        git config --global user.name 'GitHub Actions Bot'
        git config --global user.email 'actions@github.com'
        timestamp=$(date +"%Y-%m-%d_%H-%M-%S")
        mkdir -p data/history
        cp data/shop_status_results.json data/history/shop_status_results_$timestamp.json
        git add -f data/shop_status_results.json data/history/shop_status_results_$timestamp.json
        git commit -m "Update crawler results - $timestamp" || echo "No changes to commit"
        git push
