# This workflow runs the TN PDS Crawler on a schedule and stores results in the repository

name: TN PDS Crawler

on:
  # Run the crawler every day at 8:52 PM IST (15:22 UTC)
  schedule:
    - cron: '22 15 * * *'
  # Allow manual triggering
  workflow_dispatch:
  # Keep the test/lint functionality on push/PR
  push:
    branches: [ "master" ]
  pull_request:
    branches: [ "master" ]

permissions:
  contents: write  # Need write permission to commit results

jobs:
  # Test and lint job
  test:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'pull_request'
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pytest
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    - name: Test with pytest (if tests exist)
      run: |
        python -c "import pytest; import os; exit(0 if len(os.listdir('tests')) == 0 else pytest.main(['tests']))" || true
        
  # Crawler job - runs on schedule or manual trigger
  crawler:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.9
      uses: actions/setup-python@v3
      with:
        python-version: "3.9"
    - name: Install Chrome and ChromeDriver
      run: |
        sudo apt-get update
        sudo apt-get install -y wget unzip fonts-liberation libasound2 libatk-bridge2.0-0 libatk1.0-0 \
          libatspi2.0-0 libcups2 libdbus-1-3 libdrm2 libgbm1 libgtk-3-0 libnspr4 libnss3 \
          libxcomposite1 libxdamage1 libxfixes3 libxkbcommon0 libxrandr2 xdg-utils
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable chromium-driver
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install selenium webdriver-manager
    - name: Create data directory
      run: mkdir -p data
    - name: Run crawler
      run: |
        python crawai_pds_selenium.py --shop-list-json shop_list.json --output-json data/shop_status_results.json
    - name: Commit and push results
      run: |
        git config --global user.name 'GitHub Actions Bot'
        git config --global user.email 'actions@github.com'
        timestamp=$(date +"%Y-%m-%d_%H-%M-%S")
        cp data/shop_status_results.json data/shop_status_results_$timestamp.json
        git add data/shop_status_results.json data/shop_status_results_$timestamp.json
        git commit -m "Update crawler results - $timestamp" || echo "No changes to commit"
        git push
